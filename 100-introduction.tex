\chapter{Introduction}

The $ \lambda $-calculus is an abstract tool that is used in an area where mathematics meets computer science, to study algorithms, programming languages and even category theory. It was conceived by Alonzo Church, primarily as a foundation for mathematics instead of set theory or type theory \autocite{church-lambda-calculus}. A couple of years later, Church used it to show that the `Entscheidungsproblem' was unsolvable: the problem asked for an algorithm that could tell about any mathematical statement whether it was true or false, and Church showed that such an algorithm could not exist \autocite{church-number-theory}. A year later, Alan Turing showed, using previous work of Kleene \autocite{kleene-lambda}, that an algorithm is definable using the $ \lambda $-calculus if and only if it is definable using a Turing machine \autocite{turing-lambda}, solidifying the position of both the $ \lambda $-calculus and Turing machines as ways to talk about algorithms.

Later, all kinds of different flavours and extensions of the $ \lambda $-calculus were put forth, with colorful names like `simply typed $ \lambda $-calculus', `System T' and `PCF'. Even though the $ \lambda $-calculus was originally a very theoretical tool, it was also the inspiration for function programming languages, and traces of it can be seen in imperative programming languages, where unnamed functions are commonly called `lambda expressions' \autocite{java-lambdas} and are sometimes even written like \texttt{lambda x y : (x - y) * (x + y)} \autocite{python-expressions}.

Even so, the theoretical study of the $ \lambda $-calculus and its extensions continues to this day. For example, in 2017\footnote{Note that the paper has been around since 2012, when it was first published as a preprint on arXiv.}, a paper by Martin Hyland was published with the title `classical lambda calculus in modern dress' \autocite{Hyland}. In this paper, Hyland approaches the $ \lambda $-calculus from the viewpoint of universal algebra, using algebraic theories, and more generally category theory, to study it. This way, he obtains two new proofs for old theorems. The paper also contains a new theorem that shows that two different ways to study the $ \lambda $-calculus using universal algebra are equivalent.

Now, in the last century, mathematics has changed a lot. Of course, new theorems have been proved, new conjectures have been made and entire new areas of mathematics have come into existence. However, as in many professions, the arrival of the computer has affected the way that work is done in mathematics. All kinds of tools have been created that aid mathematicians in their job. Some of these tools help with quick calculations, for formulating or disproving new conjectures. Other tools help in structurally verifying ideas. There even have been some `proofs by computer', which consist of a proof on paper that a theorem can be reduced to a finite, but very large, computation, and a computer program that then executes this computation. For example, the first proofs of the four color theorem \autocite{four-color-theorem} and the Kepler conjecture \autocite{Kepler-conjecture} were done this way.

However, because the computation part of such a proof involves a lot of code, and computer code tends to contain bugs, accepting a proof by computer involves a certain amount of trust. Therefore, both of the theorems mentioned have subsequently also been proved using `computer proof assistants'\autocite{formalized-four-color-theorem}\autocite{formalized-Kepler-conjecture}. Such proof assistants, like `rocq', `lean' and `Agda', are computer programs with only a very small `trusted codebase'\footnote{Trusted codebase means the part of the codebase of which we hope that it is correct. The rest of the codebase is checked using this trusted core, so if the trusted core is correct, the rest is too.}, that can verify mathematical reasoning. In this way, if we trust the small core of such a proof assistant, and the proof assistant says that a proof is correct, then we can trust that indeed, the proof is correct.

This sounds great in theory, but in practice, the way that these programs reason about mathematics is very formal and rigid. This means that `formalizing', proving something using a proof assistant, usually involves a great amount of effort, usually much more than doing a pen-and-paper proof. However, representing this as a choice between doing pen-and-paper proofs and working with proof assistants would be too simplistic. Often, doing a pen-and-paper proof can help one to develop an intuition and get new ideas, whereas formalizing those ideas can then help to get a better view of the subtleties of a proof and to sharpen the understanding of why the proof works. Therefore, there is a lot of ongoing research into tools and best practices to make the process of formalizing as smooth as possible; to make these programs, which sometimes are experienced as `proof obstructors', really into `proof assistants'.

Now, much of contemporary mathematics is built on the foundation of set theory, which we will often refer to as \iindex{classical mathematics}. Usually, `set theory' refers to ZFC: Zermelo Fraenkel set theory with the axiom of choice, developed in the early 1900s. On the other hand, most of the formalization in proof assistants is built on the foundation of type theory. This is because computers can reason better with types than with sets, as evidenced by the many typed programming languages that are around nowadays. However, type theory has been around longer than computers have: it was initially developed by Bertrand Russell in the early 1900s, to create a foundation of mathematics that avoided Russell's paradox\footnote{``Consider the set of all sets that do not contain themself. Does this set contain itself?''}.

Nowadays, there are many flavours of type theory around. The proof assistant rocq is based on the `calculus of constructions' whereas the proof assistant Agda works with the `unified theory of dependent types'. Every flavour has its advantages and disadvantages, but in this thesis we will work with `univalent foundations', set forth by Vladimir Voevodsky \autocite{voevodsky-univalent-foundations}. In univalent foundations, the common mathematical principle that `things with a similar structure\footnote{For example, two sets with a bijection between them have a similar structure. Also, two graphs $ G $ and $ H $ have a similar structure when there is a bijection $ f $ between their sets of vertices, such that for every two vertices $ v, w : G $, we have a bijection between the set of edges from $ v $ to $ w $, and the set of edges from $ f(v) $ to $ f(w) $.}, are the same' is made explicit. The principle is used often in modern mathematics, and making this explicit in the type theory that we work with, allows us to use the principle when formalizing.

However, note that two objects can often have a similar structure in more than one way. For example, the set $ \{ \top, \bot \} $ has two bijections to the set $ \{ 0, 1 \} $. To accomodate for this, in univalent foundations, two things can sometimes `be the same' in more way than one. For example, we have two equalities between $ \{ \top, \bot \} $ and $ \{ 0, 1 \} $. This means that the concept of `sameness' is more intricate to work with in univalent foundations than it is in set theory. This is one of the reasons that there are subtleties involved in transferring definitions and proofs from set theory to univalent foundations. It is therefore still a topic of ongoing research how well different parts of set-based mathematics can be transferred to univalent foundations and what subtleties pop up when material from set-based mathematics is transferred to univalent foundations. For example, often in set-based mathematics, two definitions are equivalent if we assume the axiom of choice. However, sometimes in univalent foundations, the axiom of choice is not sufficient to make two such definitions equivalent, and then we have two different definitions. It is then interesting to explore the behaviour of these two different definitions in univalent foundations.

In this thesis, we study the paper `classical lambda calculus in modern dress' through the lens of univalent foundations, work out the details of the proofs and formalize part of the paper and its preliminaries. The major contributions are
\begin{itemize}
  \item The category theoretical preliminaries needed to understand the paper (Chapter \ref{ch:category-theory}), as well as the work of Hyland's predecessors that he expands upon (Chapter \ref{ch:previous-work}), and more detailed versions of his definitions and proofs, complemented with some examples (Chapters \ref{ch:algebraic-structures} and \ref{ch:the-paper}). This makes Hyland's paper more accessible to computer scientists.
  \item The translation of Hyland's paper and the work of his predecessors to univalent foundations (Chapter \ref{ch:the-paper}). This shows that his work can be translated to univalent foundations, and contributes to the knowledge about translating classical mathematics to univalent foundations in general.
  \item A formalization of part of the paper in rocq (Chapter \ref{ch:the-formalization}). This contributes to the knowledge about formalization in general.
  \item A new proof for the fundamental theorem of the $ \lambda $-calculus (Section \ref{sec:elementary-fundamental-theorem}). This version is easier to read and verify, because it is more elementary and uses less category theory than Hyland's proof.
  \item An analysis of the behaviour of the Karoubi envelope in univalent foundations in general (Section \ref{sec:karoubi-envelope}), and in the specific case of Paul Taylor's and Martin Hyland's work (Remark \ref{rem:difference-Taylor-Hyland}). Here we see an example of two definitions that are equivalent in classical mathematics, but become different in univalent foundations. It is interesting to see the differences between the behaviour of the two definitions in univalent foundations.
\end{itemize}

% 600-paper.tex
Now, most of this thesis works towards Chapter \ref{ch:the-paper}, about the three main proofs of Hyland's paper.
% 300-univalent-foundations.tex
Because this thesis works from a univalent point of view, Chapter \ref{ch:univalent-foundations} introduces univalent foundations and builds the preliminary knowledge in that area for the rest of the paper.
% 500-previous-work.tex
As mentioned before, the work of Hyland's predecessors that is covered in Chapter \ref{ch:previous-work} helps to understand his paper.
% 200-category-theory.tex
The category theoretical preliminaries for understanding Hyland's paper are covered in Chapter \ref{ch:category-theory}. It is probably wise to skim this chapter, and periodically come back to it to better understand the material in the other chapters.
% 400-algebraic-structures.tex
Hyland's three main theorems deal with a couple of objects that he introduces. These objects, together with some properties and examples, are introduced in Chapter \ref{ch:algebraic-structures}.
% 700-formalization.tex
Lastly, the formalization of part of the material is discussed in Chapter \ref{ch:the-formalization}.

Throughout this document, links are included to \href{https://arnoudvanderleer.github.io/cs-masters-thesis/toc.html}{\nolinkurl{documentation}} of the corresponding formalized material. The documentation refers to the state of \href{https://github.com/UniMath/UniMath/tree/5eb5c8958c4dddd4219f895bf7bc51547395522d}{\texttt{the UniMath repository}} at commit \texttt{5eb5c8958c4dddd4219f895bf7bc51547395522d}.
