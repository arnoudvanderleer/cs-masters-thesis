% 996 0; 447 37; 1383 1017; 26 26; 6313 889; 2562 2332; 1255 326; 3015 1; 4206 1668; 940 48; 9 5; 508 4; 1701 1267; Total: 23361 7620
% 434 292; 1144 58; 96 0; 469 113; 145 42; 495 90; 508 221; Total: 3291 816
\chapter{The formalization}\label{ch:the-formalization}
In addition to the reading, investigating and writing mathematics that lead up to the previous chapters, this thesis project also had a formalization component. Parts of Hyland's paper were carefully written out in detail in a proof assistant and added to a library of formalized mathematics with the univalent point of view, called \textit{UniMath}.

This chapter will give an overview of what was formalized, as well as point out and evaluate a couple of design decisions that were made.


\section{Some numbers}
The code for this project was written over the course of about 18 months, spread over \href{https://github.com/UniMath/UniMath/pulls?q=is%3Apr+author%3Aarnoudvanderleer+is%3Aclosed}{20 pull requests}: 13 with content about Hyland's paper, adding 23.361 and removing 7.620 lines of code, and 7 with only some missing category theoretical preliminaries, adding 3.291 and removing 816 lines.

\section{Overview of the formalized material}
The material that was formalized can be subdivided into material that is introduced or described in Hyland's paper, and category theoretical preliminaries that are necessary to make the proofs in Hyland's paper work.

The formalized parts of Hyland's paper are collected in a package in the library called \textit{algebraic theories}. This package now consists of over 14.000 lines of code, and contains
\begin{itemize}
% AlgebraicTheoryCategory.v AlgebraicTheoryCategoryCore.v AlgebraicTheories.v AlgebraicTheoryMorphisms.v
% LambdaTheories.v LambdaTheoryCategory.v LambdaTheoryCategoryCore.v LambdaTheoryMorphisms.v
% AlgebraCategory.v AlgebraCategoryCore.v Algebras.v AlgebraMorphisms.v
% PresheafCategory.v PresheafCategoryCore.v PresheafMorphisms.v Presheaves.v
  \item Definitions for algebraic theories, $ \lambda $-theories, algebras and presheaves of algebraic theories, together with their morphisms and their categories. Proofs that the categories are univalent, that the categories of algebraic theories, $ \lambda $-theories and presheaves have limits and that the categories of algebras and presheaves are fibered over the category of algebraic theories.
% FreeTheory.v LambdaCalculus.v OnePointTheory.v ProjectionsTheory.v TheoryAlgebra.v
  \item The terminal algebraic theory, the free algebraic theory on a set, with as a special case the initial algebraic theory, the $ \lambda $-theory $ \Lambda $, the $ T $-presheaf structure on $ T $ and the $ T $-algebra structure on $ T_n $.
% FreeMonoidTheory.v FreeObjectTheory.v
  \item The `free object' algebraic theory with a functor from $ C $ to its algebras. Also the equivalence between the `free monoid' algebraic theory $ T $, together with a proof that the functor from monoids to $ T $-algebras is an equivalence.
% EndomorphismTheory.v
  \item The construction of the endomorphism theory $ E(X) $ for some object $ X : C $ in a category $ C $ with finite products.
% Combinators.v CategoryOfRetracts.v OriginalRepresentationTheorem.v
  \item The original version of Scott's representation theorem.
% Plus1Presheaf.v RepresentationTheorem.v
  \item Hyland's version of Scott's representation theorem, including the construction of the presheaf $ A(P, 1) $ and the proof that it is the exponential object $ P^T $.
% AlgebraToTheory.v
  \item The construction of a $ \lambda $-theory from a $ \Lambda $-algebra $ A $ as the endomorphism theory $ E_{\RAct{A_1}}(U_A) $ (Definition \ref{def:lambda-algebra-to-lambda-theory}).
% AlgebraicTheoryToLawvereTheory.v
  \item The construction of a Lawvere theory $ L $ from an algebraic theory $ T $, and the equivalence between the presheaf category $ PL $ and the presheaf category $ \Pshf T $.
% ExtensionsTheory.v LambdaCalculus.v
  \item The theory of extensions $ T_A $ of a $ T $-algebra.
  \item An axiomatic definition of the pure $ \lambda $-calculus (see Section \ref{sec:axiomatic-lambda-calculus}).
\end{itemize}

Since Hyland's paper uses a lot of general category theory, formalizing it entailed adding the preliminaries that had not yet been formalized to the library. Among these are:
\begin{itemize}
  \item Univalence of the Sigma displayed category.
  \item The Sigma displayed category creates limits.
  \item The definitions $ \overline C $ and $ \hat C $ for the Karoubi envelope and their preliminaries.
  \item The contents of Section \ref{sec:monoid-category}: The construction of a one-object category from a monoid, the equivalence between it's presheaves and sets with a right monoid action, properties of the category of sets with a monoid action and restriction and extension of scalars.
  \item The Yoneda embedding preserves exponential objects.
  \item The uniqueness of the Rezk completion.
% FundamentalTheorem/SurjectivePrecomposition.v
  \item If $ F : C \to C^\prime $ is a fully faithful functor, and $ D $ is a category with colimits, then the precomposition functor $ F \bullet - : [C^\prime, D] \to [C, D] $ is split essentially surjective.
  A precomposition functor is equivalent if ...
  \item A proof of a generalized version of Lemma \ref{lem:equivalent-presheaf-cats}, showing that if $ F : C \to C^\prime $ is a functor, if $ F^\prime : C^\prime \to C^{\prime \prime} $ is a fully faithful functor, if $ D $ is a category with colimits and if the precomposition $ (F \bullet F^\prime) \bullet - : [C^{\prime \prime}, D] \to [C, D] $ is an adjoint equivalence, then $ F^\prime \bullet - $ and $ F^\prime \bullet - $ are adjoint equivalences too.
% FiniteSetSkeleton.v
  \item The skeleton category $ F $ of finite sets, with
  \[ F_0 = \{ 0, 1, \dots \} \quad \text{and} \quad F(m, n) = \SET(\{ 1, \dots, n \}, \{ 1, \dots, m \}). \]
% IndexedSetCategory.v
  \item The univalent category $ \SET^A $ of indexed sets $ (X_a)_a $ over a type $ A $, which can be defined formally as
  \[ \SET^A_0 = (A \to \SET) \quad \text{and} \quad \SET^A(X, Y) = \prod_a X_a \to Y_a. \]
\end{itemize}

\section{Equality, Isomorphisms and Equivalences}
One of the lessons during this project was that for formalizing, it is always important to choose te right equality for the job. For example, for two categories $ C $ and $ C^\prime $, one can aim to prove either that $ C = C^\prime $ or that we have a functor $ F : C \to C^\prime $ that has a right and left inverse, or that we have a functor $ F : C \to C^\prime $ that is an adjoint equivalence. Even though for univalent categories, these three notions all coincide, in practice it is really hard to show equality of categories directly: To show $ C = C^\prime $, we would need to show that we have an equality $ H : C_0 = C_0^\prime $, and then that for all $ c, c^\prime : C $
\[ (C(\mathrm{transport}_H(c), \mathrm{transport}_H(c^\prime))) = C^\prime(c, c^\prime). \]
It is usually much easier to show that there exists a functor $ F $ with an inverse, because then we don't have to show these equalities of types, but can suffice with maps between the types and equalities of the objects and morphisms that are mapped. However, for the morphisms we still have transports.

The easiest option is usually to show that we have an adjoint equivalence. That way, we don't have to show equality of objects, but only isomorphism (even though in a univalent category, that is of course the same). Then we only have to show equality of morphisms, and this is usually very doable.

An example of this is the proof that for the algebraic $ T $ where $ T_n $ is the free monoid on $ \{ x_1, \dots, x_n \} $, the $ T $-algebras are equivalent to monoids. Initially, this was a proof about (weak) equivalence (so under univalence, equality) of the object types, but of course, this says nothing about morphisms, so it is incomplete. However, it was possible to remove parts of the proof, and use the rest to construct an adjoint equivalence.

In general, the formalization is the cleanest and the easiest when we use equality for elements of homotopy sets (morphisms, or terms in an algebraic theory, for example), isomorphisms for objects in a category and adjoint equivalences for categories themselves.

\section{Displayed categories}\label{sec:displayed-categories}

\index{displayed category}\textit{Displayed categories} (introduced in \autocite{displayed-categories}) are a mathematical idea which provides a great tool in formalizing categories. One of the motivations behind displayed categories is the fact that mathematicians often define a category in terms of another: a monoid is a set together with a binary operation, a group is a monoid in which every element has an inverse, a topological space is a set together with a chosen collection of subsets.

Traditionally, one would say in such a case that we have a `forgetful functor' $ F : C^\prime \to C $. However, working with displayed categories has some advantages over this older approach, both conceptually and practically.

A displayed category $ D $ over a category $ C $ firstly consists of a type $ D_c $ for every $ c : C $ (corresponding to the type of objects $ F^{-1}(c) $) and a type $ d \to_f d^\prime $ for all $ d: D_c $, $ d^\prime : D_{c^\prime} $ and $ f : C(c, c^\prime) $ (corresponding to $ C^\prime((c, d), (c^\prime, d^\prime)) $). A displayed category also consists of an identity `morphism' $ d \to_{\id c} d $ for all $ d : D_c $, and compositions $ \bar f \cdot \bar g : d \to_{f \cdot g} d^{\prime \prime} $ for $ \bar f : d \to_f d^\prime $ and $ \bar g : d^\prime \to_g d^{\prime \prime} $.

When we have a displayed category $ D $ over $ C $, we can define two categories. First of all, we can form the \iindex{total category} $ \int_C D $ (which corresponds to the category $ C^\prime $ in the forgetful functor example), consisting of pairs $ (c, d) $ with $ d : D_c $. Then the forgetful functor is given by $ \pi_1 : \int_C D \to C $. Also, for every $ c : C $, we can form the \iindex{fiber category} over $ c $, which we will also denote as $ D c $. The objects here are the displayed objects over $ c $, and the morphisms are the displayed morphisms over $ \id c $. In the forgetful functor example, this is the preimage category $ F^{-1}(c) $. Lastly, note that if we have a displayed category $ D $ over $ C $, and a displayed category $ E $ over $ \int_C D $, then we can form the \iindex{sigma displayed category} $ \sum_D E $ over $ C $, where $ (\sum_D E)_c $ consists of pairs $ (d, e) $ with $ d : D_c $ and $ e : E_{(c, d)} $. Of course, the total categories are equivalent: $ \int_{\int_C D} E \cong \int_C (\sum_D E) $, which boils down to rebracketing $ ((c, d), e) $ to $ (c, (d, e)) $.

One of the reasons why displayed categories are so useful when formalizing, is the fact that for every next category, only the `new' parts have to be defined. If we define a category that consists of `functors with some additional data', we do not again have to show what the identity and composition functors are, but instead it suffices to construct the additional data for those functors. Also, displayed categories come in handy because some properties of $ \int D $ that are hard to prove directly, can be derived from properties of $ C $ and the fiber categories $ D_c $. For example, we can show that the category of groups is univalent, by noting that $ \SET $ is univalent, and by furthermore showing that the fibers of the displayed category of monoids over $ \SET $ and the fibers of the displayed category of groups over the category of monoids, are univalent.

\subsection{The categories in question}\label{subsec:formalized-categories}

Here is a diagram of the displayed structure of the categories of algebraic theories, $ \lambda $-theories, algebraic theory algebras and algebraic theory presheaves, every arrow denotes that a category is displayed over the total category of the next one.
\begin{center}
  \begin{tikzcd}
    \text{algebra} \arrow[d] & \lambda\text{-theory} \arrow[d] & \text{presheaf} \arrow[d] \\
    \text{algebra data} \arrow[d] & \lambda \text{-theory data} \arrow[d] & \text{presheaf data} \arrow[d] \\
    {\text{(algebraic theory, set)}} \arrow[r] & \text{algebraic theory} \arrow[d] & {\text{(algebraic theory, indexed set)}} \arrow[l] \\
    & \text{algebraic theory data}\arrow[d] &\\
    & \text{indexed set} &
  \end{tikzcd}
\end{center}
In the end, they all derive from the category of indexed sets over the natural numbers (or equivalently, the category of sequences of sets).

Note that every `relevant' category is constructed in two stages: in the first step, in a category called `\dots data', the displayed objects give the structure (the algebraic theory variables and substitution, the algebra action or the presheaf action) of the objects in question, and the displayed morphisms preserve this structure. In the second step, we take the full subcategory, consisting of the objects that satisfy the right properties. For algebraic theories, these properties are the axioms about interplay between the substitution and the variables.

Note that the algebra data and presheaf data categories are displayed over a product category, which is displayed over the category of algebraic theories (see Subsection \ref{subsec:displayed-product}).

The reason why we first construct the category of all algebraic theory algebras together, is because we need we need it to show that that algebras are fibered over algebraic theories (see Subsection \ref{subsec:fibrations}). In fact, we need the category of algebras as a displayed category over the category of algebraic theories. Since in the construction given above, it is displayed over the category of algebra data, we use the sigma construction twice, to bundle all the algebra information in a displayed category of one layer over algebraic theories. Then the category of $ T $-algebras can be defined as the fiber over $ T $ of this displayed category. For presheaves, it is the same story.

Even though the approach of first defining the categories of all algebras or presheaves and then taking a fiber of this, is necessary to talk about fibrations, there is a drawback to this approach. Morphisms in any fiber category are the displayed morphsims over $ \id T $. Therefore, naively composing two morphisms in our fiber gives a displayed morphism over $ \id T \cdot \id T $ and we need to transport over the equality $ \id T \cdot \id T = \id T $ to get morphisms in our fiber category again. So even though we can prove that, as expected, $ (f \cdot g)(a) = g(f(a)) $ for $ f: \Alg T(A, B) $, $ g: \Alg T(B, C) $ and $ a : A $, this is no longer a definitional equality.

\subsection{Univalence}
All of the categories in the diagram above are univalent. The proofs of this proceed by reducing to known or easy cases. For example, we know that a full subcategory of a univalent category is again univalent because it inherits isomorphisms and equalities. Also, we know that a product of univalent categories is univalent again, since its isomorphisms and equalities are equivalent to pairs of isomorphisms and equalities of its factors. We already knew that $ \SET $ is univalent, and in this project, the category of sets, indexed over a type, was constructed and shown to be univalent.

The most interesting proofs are for the `\dots data' categories. These proofs reduce to univalence of the underlying category (so we prove univalence layer by layer) and univalence of the fiber categories, so for example the fiber of all algebraic theory data structures on one indexed set $ (T_n)_n $. Showing that this fiber is univalent, means showing that for any two choices of substitution functions $ f_n, g_n: T_m \times T_n^m \to T_n $ and variables $ x_{i, n}, y_{i, n} : T_n $, there is an equivalence between `$ (f_n)_n = (g_n)_n $ and $ (x_{i, n})_{i, n} = (y_{i, n})_{i, n} $' and `the identity on $ T_n $ commutes with $ f_n $ and $ g_n $, and with the $ x_{i, n} $ and $ y_{i, n} $'. Since these are mere propositions and they imply each other, we indeed have this equivalence.

\subsection{Fibrations}\label{subsec:fibrations}

One of the places where displayed categories are conceptually better to work with than forgetful functors, is in the case of fibrations. Recall that a functor $ P: C^\prime \to C $ is a fibration if for every $ Y : C^\prime $ and $ f : C(X, P(Y)) $, there exists $ \bar X : C^\prime $ with $ P(\bar X) = X $ (and a cartesian morphism $ \bar f : C^\prime(\bar X, Y) $ with $ P(\bar f) = f $). The equality $ P(\bar X) = X $ is on objects in the category, and this violates the \textit{principle of equivalence}: `if something is true for $ A $, and $ A $ is isomorphic to $ B $, then it should also hold for $ B $'. Of course, if the category is univalent, isomorphism and equality are the same, but definitions that use equality on objects still give a bit of conceptual friction.

However, we can also define this in the language of displayed categories. The definition becomes
\begin{definition}
  A displayed category $ D $ over $ C $ is a fibration if for every $ \bar Y : D Y $ and $ f: C(X, Y) $, we have $ \bar X : D X $ and a cartesian morphism $ \bar f : \bar X \to_f \bar Y $.
\end{definition}
This avoids using equality on objects, because we can just talk about `the objects above $ X $'. Therefore, in UniMath fibrations are defined in this way and the algebraic theories package uses this definition to show that the displayed categories of algebraic theory algebras and presheaves, over the category of algebraic theories, are fibrations.

\subsection{Limits}
In this subsection, we will mainly treat binary products, as they are somewhat simpler to understand than limits in general. However, it is not too hard to generalize the material presented here to limits in general, and the formalized proofs treat limits in general, instead of binary products.

Recall from Remark \ref{rem:algebraic-theory-limits} that algebraic theories have all limits, and that given a diagram, the underlying set of the limit is the limit of the underlying sets. Compare this to group or ring theory: for rings $ R $ and $ S $, the set $ R \times S $ can again be given a ring structure, which is the binary product of $ R $ and $ S $ in the category of rings.

This is all very reminiscent of the way that displayed categories `borrow' information from their base category. Now if a category $ C $ has binary products, we say that a displayed category $ D $ over $ C $ \textit{creates binary products}, if for all $ \bar X : D_X $, $ \bar Y : D_Y $, we can `lift' their product. That is, if we can find
\[ \overline{X \times Y} : D_{X \times Y}, \quad \bar{\pi_1} : \overline{X \times Y} \to_{\pi_1} \bar X \quad \text{and} \quad \bar{\pi_2} : \overline{X \times Y} \to_{\pi_2} \bar Y \]
and if $ (X \times Y, \overline{X \times Y}) $ with the projections $ (\pi_1, \bar{\pi_1}) $ and $ (\pi_2, \bar{\pi_2}) $ is the product of $ (X, \bar X) $ and $ (Y, \bar Y) $ in $ \int_C D $. By definition, if $ D $ creates binary products, then $ \int_C D $ has binary products.

In itself, this is not a very revolutionary idea. However, when formalizing, this allows us to work layer by layer, every time adding a little bit of new information, and reusing the rest from the layer below to show that every category that we construct has binary products. Also, in the rest of this section we will see that very often, it suffices to formalize a lot less than the full construction of a binary product, and that when we do constructions on displayed categories, like the sigma displayed category or taking a fiber category, we often immediately deduce that the resulting (displayed) category still has (or creates) binary products.

During this project, a small lemma was added to the library, showing that for a full subcategory $ \int_C D \subseteq C $, where $ D_c $ is a mere proposition for all $ c : C $, if for all $ (X, \bar X), (Y, \bar Y) : \int D $, we have $ D_{X \times Y} $ (the $ C $-product of two objects in $ \int D $ is again in $ \int D $), then $ D $ creates binary products. Also, a lemma was added that shows that if a displayed category $ D $ over $ C $, and a displayed category $ E $ over $ D $, both create binary products, then $ \sigma_D E $ also creates binary products.

Note that for many categories, the displayed morphisms are mere propositions. In particular, this holds for the `\dots data' categories of this project. For example, an algebra morphism is a function that `respects the operation', and respecting the operation is a mere proposition. In these categories, it is not necessary to give a full proof that the object $ (X \times Y, \overline{X \times Y}) $ and the morphisms $ (\pi_i, \overline{\pi_i}) $ form a binary product, because it suffices to show that for all $ C, \bar C, f, g $ and all $ \bar f : \bar C \to_f \bar A $ and $ \bar g : \bar C \to_g \bar B $, we can lift the product morphism to
\[ \overline{\langle f, g \rangle} : \bar C \to_{\langle f, g \rangle} \overline{A \times B} \]
For example, if the function $ f $ commutes with the algebra actions on $ \bar C $ and $ \bar A $, and $ g $ commutes with the actions on $ \bar C $ and $ \bar B $, then we need to show that the product morphism $ \langle f, g \rangle $ commutes with the actions on $ \bar C $ and $ \overline{A \times B} $. A lemma showing that it suffices to show that we can lift the product morphism was added to the library, and used to show that the `\dots data' displayed categories create binary products.

Now, recall that the categories of $ T $-algebras and $ T $-presheaves are a fiber of a displayed category. Now, if we want a binary product of $ X, Y : \Alg T $ and we take the product of $ (T, X) $ and $ (T, Y) $ in the category of all algebras, we end up with $ (T \times T, X \times Y) $, or $ X \times Y : \Alg {T \times T} $, even though we would like to have $ (T, X) $ for some $ X : \Alg T $. It turns out that here we need the fact that algebras and presheaves are fibered over algebraic theories, and adding the following lemma to the library gave the final brick for showing show that all categories discussed here have limits:
\begin{lemma}
  For a displayed category $ D $ over a category $ C $, suppose that $ C $ has binary products, that $ D $ creates binary products, and that $ D $ is a fibration. Then the fiber categories $ D_X $ have binary products.
\end{lemma}
\begin{proof}
  Take $ \bar X_1, \bar X_2 : D_X $. We have a product $ \bar X_1 \times \bar X_2 : D_{X \times X} $ with projections $ \bar \pi_i : \bar X_1 \times \bar X_2 \to_{\pi_i} \bar X_i $. For the diagonal morphism on $ X $, the fibration gives an object $ \bar Y $ and a a cartesian lift:
  \[ \bar f : \bar Y \to_{\langle \id X, \id X \rangle} \bar X_1 \times \bar X_2, \]
  so we have projections
  \[ \bar p_i = \bar f \cdot \bar \pi_i : \bar Y \to_{\id X} \bar X_i. \]
  In the diagram below, the first row is the fiber $ D_X $ and the second row is the fiber $ D_{X \times X} $.
  \begin{center}
    \begin{tikzcd}
      \bar X_i & \bar Y \arrow[d, "\bar f" description] \arrow[l, "\bar p_i"] & \bar Z \arrow[ll, "\bar g_i"', bend right] \arrow[l, "\bar h"] \arrow[ld, "{\langle \bar g_1, \bar g_2 \rangle}"] \\
      & \bar X_1 \times \bar X_2 \arrow[lu, "\bar \pi_i"] &
    \end{tikzcd}
  \end{center}
  Now, given some $ \bar g_1: \bar Z \to_{g_1} \bar X_1 $ and $ \bar g_2: \bar Z \to_{g_2} \bar X_2 $, we have
  \[ \langle \bar g_1, \bar g_2 \rangle : \bar Z \to_{\langle g_1, g_2 \rangle} \bar X_1 \times \bar X_2. \]
  Because $ \bar f $ is cartesian, we have a unique
  \[ \bar h: \bar Z \to_{\id X} \bar Y, \]
  such that
  \[ \bar h \cdot \bar f = \langle \bar g_1, \bar g_2 \rangle. \]
  Therefore,
  \[ \bar h \cdot \bar p_i = \bar h \cdot \bar f \cdot \bar \pi_i = \langle \bar g_1, \bar g_2 \rangle \cdot \bar \pi_i = \bar g_i, \]
  which shows that $ \bar Y $ is the product of $ \bar X_1 $ and $ \bar X_2 $ in $ D_X $.
\end{proof}

\subsection{Chicken or egg?}
The formalization started with definitions for the objects and morphisms in question: algebraic theories and algebras. Using these, the categories were defined directly. However, since showing univalence is much easier when working with displayed categories, the definitions of the categories were decoupled from the definitions of the objects and morphisms, and instead were constructed as displayed categories. This meant that of every definition, a part was duplicated: once for the object (and morphism) types, and once for the objects and morphisms in the displayed categories. This meant that it was in theory possible to get a mismatch between, for example, the definition of algebraic theories and their category.

Of course, it was possible to get rid of the objects altogether, and just define things in terms of the objects and the morphisms of the categories. However, in practice this causes problems because of \textit{coercions}. For example, if $ X $ is an algebraic theory, mathematicians like to use the name $ X $ also to denote the sequence of sets $ X_n $. Under the hood, this uses a coercion, which allows one to use the same symbol to denote both the entire object, or a part of it, depending on the context. However, when working with displayed categories, the category of algebraic theories is displayed over the category of algebraic theory data, which is displayed over the category of sequences of sets. It turns out that in coq, coercions on categories do not compose very well: If $ X $ is an algebraic theory, and we have coercions from algebraic theories to algebraic theory data, and from algebraic theory data to sequences of sets, we can still only use $ X $ to denote the algebraic theory data, and not the sequence of sets.

In the end, the solution was to first define the category, and then the object and morphism types as the objects and morphisms of the category. The coercions can then be defined on the standalone object and morphism types, which works very well.

\subsection{A product of categories}\label{subsec:displayed-product}
Given two categories $ C $ and $ C^\prime $, their product $ C \times C^\prime $ can be viewed as a displayed category over $ C $ (or $ C^\prime $), where the objects over any object are the objects of $ C^\prime $, and the displayed morphisms $ g: c \to_f c^\prime $ are the morphisms $ g: C^\prime(c, c^\prime) $. There are two ways to formalize this.

The first approach uses \textit{reindexing}, by taking the unique functor to the unit category $ F : C \to \{ \star \} $, and considering $ C^\prime $ as a displayed category $ D $ over $ \{ \star \} $. Then we have the reindexed (pullback) displayed category $ F^* D $, with $ (F^* D)_c = D_{F(c)} $, with $ \int (F^* D) \cong C \times C^\prime $. The advantage of this approach is that it uses fairly simple general machinery. The disadvantage is that for the general construction of $ (F^* D) $, we need to transport over the equalities
\[ \id{F(X)} = F(\id X) \quad \text{and} \quad F(f) \cdot F(g) = F(f \cdot g) \]
in the definitions of, respectively, the displayed identity and composition. In practice, this adds friction to the formalization.

The second approach is by constructing the displayed category $ D $ directly, setting $ D_c = C^\prime $ and $ c \to_f c^\prime = C^\prime(c, c^\prime) $. This is slightly more work, because we do it in an elementary way instead of using category theoretical machinery. In return, this approach gives cleaner definitions in practice for the identity and composition morphisms of the total category. Therefore, halfway during the project, a switch was made from the first to the second approach.

\section{Duplication in definitions}
One of the adages in software engineering is `DRY': ``Don't Repeat Yourself'': Long expressions and blocks of code that occur multiple times throughout the program should usually be abstracted into a separate function. One of the reasons for this is that it is easier to change code in this function, than to change every instance of the repeated expression or block of code. When writing mathematics, such functions are usually called `lemmas'. However, in this project there was also another example of duplication, which occurred in statements of definitions and lemmas. For example, in the definition of the displayed category of algebraic theories, the displayed object type over a sequence of sets $ T_n $ is
\[ \left(\prod_n \prod_{i : \{ 1, \dots, n \}} T_n \right) \times \left( \prod_{m, n} T_m \times T_n^m \to T_n \right), \]
corresponding to the variables and the substitution. Then, the constructor of an algebraic theory takes arguments
\[ v : \prod_n \prod_{i : \{ 1, \dots, n \}} T_n \quad \text{and} \quad s : \prod_{m, n} T_m \times T_n^m \to T_n. \]
Also, given an algebraic theory, we have accessors:
\[ x_{n, i} : T_n \quad \text{and} \quad \bullet_{m, n} : (\prod_{m, n} T_m \times T_n^m \to T_n) \]
Lastly, when we define a new algebraic theory, we need to provide terms of these types again.

Therefore, in this project, the type of every one of these components is given a name, ending in \texttt{\_ax}, for `axiom'. For example, there are \texttt{var\_ax} and \texttt{subst\_ax} for algebraic theories, \texttt{mor\_var\_ax} and \texttt{mor\_subst\_ax} for their morphisms, \texttt{action\_ax} for algebras and \texttt{app\_ax} and \texttt{abs\_ax} for $ \lambda $-theories. The definition of the displayed categories, the constructors, the accessors and the definitions of new objects can then refer back to this.

This indeed reduces the amount of duplication in the code, and makes it slightly easier to write some definitions, because one does not have to remember and type the exact formulation of every axiom. However, there is no free lunch here: The axioms are not immediately unfolded when they occur. Therefore, if one uses the constructor for an algebraic theory, the goals become \texttt{var\_ax} and \texttt{subst\_ax}, and these have to be unfolded to see what they mean. Also, when coq is asked to state the property of an algebra morphism, it responds with \texttt{mor\_action\_ax}, which is not very informative. This is not a big problem, per se, but it adds some friction when formalizing. This friction could be reduced a lot if coq would have some sort of macros, which would immediately be unfolded upon use, and would never be printed.

Another drawback of this approach is that for algebras and presheaves, there are in fact two different axioms. This is because their categories are displayed over the categories of algebraic theories, and the category of $ T $-algebras and $ T $-presheaves are fibers of the full displayed category. Therefore, the morphisms of $ T $-algebras are given by displayed morphisms over $ \id T $, so the axiom of an algebra morphism $ g $ is
\[ \mathtt{mor\_action\_ax}: \prod_n \prod_{t : T_n} \prod_{a : A^n} g (t \bullet a) = \id T(t) \bullet (g (a_i))_i, \]
and in practice, this makes it harder to work with them. There was an instance where one conversion from $ \id T(t) $ to $ t $ added multiple seconds to the compilation time. Therefore, for the algebras and presheaves, some of these axioms have to be stated twice: once to define the displayed category, and once for most of the other occurrences.

\section{Tuples}

A lot of the mathematics in the paper requires us to work with `tuples': terms that bundle a certain number of terms of some type. A tuple type already occurs already as $ T_m^n $ in the definition of the substitution operation of algebraic theories:
\[ \bullet: T_m \times T_n^m \to T_n. \]
Now, there are two common ways to formalize such tuples, both with their advantages and disadvantages.

The first option is to say that the type $ A^n $ just denotes the $ n $-fold cartesian product
\[ A \times (\dots \times (A \times A) \dots). \]
There are multiple advantages to this approach. First of all, this approach allows us to easily construct `literals', like $ (5, -12) : \mathbb Z^2 $ or $ (\bot, \top, \top) : \mathrm{bool}^3 $. It is also very easy to extend an $ n $-tuple $ a $ with another element $ x $, because this just gives the pair $ (x, a) $. Because of the clear relation between the extended tuple and the original one, this approach also allows for a nice induction principle: a way to prove things about general $ a : A^n $ by proving it for $ () : A^0 $ and by showing that if it holds for any $ a : A^m $, it also holds for $ (x, a) : A^{m + 1} $ for all $ x $ (note that this is a version of \texttt{fold} for lists).

The other approach is to view $ A^n $ as the type of functions from $ \{ 1, \dots, n \} $ to $ A $. An advantage of this is that it becomes trivial to extend a function $ f $ on $ A $ to a function on its tuples, sending $ a $ to $ f \circ a $ (which would be called $ (f(a_i))_i $ in this thesis). Also, this approach makes it very easy to define $ n $-tuples of arbitrary size. For example, $ (x_i)_i : T_n $ (in one of the axioms of an algebraic theory) is just the function that sends $ i $ to the variable $ x_i $. These tuples of arbitrary size have very good computational behaviour, since the value of $ (x_i)_i $ at $ j $ is, by definition, $ x_j $.

Very early on in the project, a choice was made to use the second approach. Since many definitions require extending a mapping to a tuple, and since we don't very often need to extend tuples or do induction on a tuple, this proved to be the right choice.

The main place where we need to be able to extend tuples, is when working with $ \lambda $-theories, for example in the axioms about the relation between substitution and $ \lambda $-abstraction and -application. To accomplish this, we use the equivalence between $ \{ 1, \dots, n + 1 \} $ and $ \{ 1, \dots, n \} \sqcup \{ \star \} $.

Also, when working with a $ \Lambda $-algebra $ A $ (or an algebra) for the free monoid theory, we often want to define an operation, for example $ \circ $, on $ A $ by sending $ a $ and $ b $ to $ (x_1 \circ x_2) \bullet (a, b) $. Since constructing the literal tuple $ (a, b) $ directly as a function quickly becomes a mess, we use the equivalence between $ A \times \dots \times A $ and $ \{ 1, \dots, n \} \to A $ to define literal tuples via the cartesian product, and then transform them to functions.

Using some lemmas about the behaviour of extended tuples and literals defined this way, this slightly mixed approach works fine. However, it would be worth investigating whether it is possible to have a tuple type in the library with all of the operations of both the function and the cartesian product approaches, so that it is no longer necessary to choose between them.

\section{Products}
A very similar problem pops up in the proof of the representation theorem, where the definition of the endomorphism theory $ E_{\Pshf L}(L) $ meets the isomorphism $ \Pshf L(L^n, L) \cong L_n $ of the Yoneda lemma for presheaves. First of all, recall that the function $ \lambda_n $ of $ E(L) $ is defined via the following chain of morphisms
\[ \Pshf L(L^{n + 1}, L) = \Pshf(L^n \times L, L) \cong \Pshf L(L^n, L^L) \xrightarrow{- \cdot \text{abs}} \Pshf L(L^n, L). \]
To make this easy, we would like to have $ L^{n + 1} = L^n \times L $, and this would give a definition $ L^n = ( \dots (I \times L) \times \dots) \times L $ where $ I $ is the terminal object.

On the other hand, recall that inverse morphism of $ \Pshf L(L^n, L) \cong L_n $ sends $ s: L_n $ to the presheaf morphism that sends $ (t_i)_i : L^n_m $ to $ s \bullet t : L_m $. Here, we would like the sets of $ L^n $ to match the tuple types in the definition of the substitution $ \bullet : L_n \times L_m^n \to L_m $. However, these two are incompatible, since elements of the repeated product are nested pairs, whereas the tuples in $ L_m^n $ are functions from $ \{ 1, \dots, n \} $ to $ L_m $.

In this case, somewhat of a compromise was made. The endomorphism theory was indeed defined using a repeated binary product, and the construction of finite powers from the terminal object and binary products\footnote{Unfortunately, defininig the product of a tuple of objects was much harder than defining the finite power of one object. When generalizing the approach to tuples of objects, some terms suddenly do not have the `correct' type, which has something to do with some equalities like $ ((x_1, \dots, x_n) + (y))_{n + 1} = y $ that do not hold definitionally. Even though this specific problem can be solved using transports, the rest of the proof becomes then much harder because the transports get in the way}. On the other hand, the Yoneda lemma for $ L $-presheaves does indeed use the `usual' notion of products $ L^n $ where the sets are tuple types. Luckily, the library contains a proof that two products are isomorphic, so where the definitions clash, we use the isomorphisms to translate between them. Even though this adds friction, in the end it is pretty straightforward.

Also, this project added a lemma to the library stating that in any category $ C $, $ \prod_{i : \emptyset} X_i $ (for $ X : \emptyset \to T $ a family of objects) is given by the terminal object. However, the repeated binary product construction needed $ \prod_{i : \{ i \mid 1 \leq i \leq 0 \}} X_i $, and $ \emptyset $ is not definitionally equal to $ \{ i \mid 1 \leq i \leq 0 \} $. Initially, this was solved by transporting along the equality $ \emptyset = \{ i \mid 1 \leq i \leq 0 \} $. However, much further down the line, some proof involved an element of this product over $ \{ i \mid 1 \leq i \leq 0 \} $, and because of the transport, coq was not able to see that this product was just $ \{ \star \} $. Therefore, the statement of the lemma about the empty product was changed to ``For a type $ I $ and a family $ X : I \to C $, if we have a function $ f : I \to \emptyset $, the terminal object gives the product $ \prod_{i : I} X_i $''. Note that having the function $ f $ is equivalent to $ I $ being equivalent (and therefore equal) to the empty type. This change in the statement did provide the right amount of generality to make computation down the line much smoother.

\section{The \texorpdfstring{$ n + p $}{n + p}-presheaf}
Now, the definition of the endomorphism $ \lambda $-theory $ E_{\Pshf L}(L) $ requires that the theory presheaf $ L $ is exponentiable. As shown before, $ P^L $ is given by $ A(P, 1) $, the `plus 1' presheaf, where the last variable is ignored in the $ L $-action. In fact, one can show that $ P^{L^n} $ is given by $ A(P, n) $ and this was the level of generality at which the statement was formalized. To obtain the action
\[ \bullet : A(P, p)_m \times L_n^m \to A(P, p)_n, \]
the equivalence
\[ \{ 1, \dots, n + p \} \simeq \{ 1, \dots, n \} \sqcup \{ 1, \dots, p \} \]
from the library was used, which sends the first $ n $ elements to the first set, and the last $ p $ elements to the second, so this could indeed be used to construct elements of tuples $ A^{n + p} $, separating them into first a tuple of $ n $ and then a tuple of $ p $. After this, it was time to construct the presheaf morphisms for $ \rho $ and $ \lambda $ between $ L $ and $ A(L, 1) $, but this turned out to be much harder than expected, because $ A(L, 1)_n = L_{n + 1} $, whereas $ \rho $ and $ \lambda $ are functions between $ L_n $ and $ L_{S n} $, where $ S n = 1 + n $. Unfortunately, $ 1 + n $ is not definitionally equal to $ n + 1 $, and even though we might transport along a proof $ h: 1 + n = n + 1 $, this will give problems down the road.

In this case, the cleanest solution was to only do the construction for the special case $ p = 1 $ and take $ A(P, 1)_n = P_{S n} $, using the equivalence from the library
\[ s_m: \{ 1, \dots, n + 1 \} \simeq \{ 1, \dots, n \} \sqcup \{ \star \}, \]
with
\[ s_m(i) = \left\{ \begin{array}{cc}
    i & i < m\\
    \star & i = m\\
    i - 1 & i > m
\end{array}\right. ,  \]
taking $ m = n $ in this case.

It is possible that constructing a new equivalence
\[ \{ 1, \dots, p + n \} \simeq \{1, \dots, n\} \sqcup \{1, \dots, p\}, \]
makes it possible to generalize the result to general $ p $ again. However, this equivalence would be somewhat weird, because the order of $ n $ and $ p $ are reversed. Also, proving things about these standard finite sets is quite hard, and outside the scope of this project, so this was not attempted in this project, but potentially left for future work.

\section{Quotients}\label{sec:quotients}
This project used quotients twice, once explicitly and once implicitly. The first occurrence of quotients was the construction of the `extension of scalars' functor $ f_*: \RAct N \to \RAct M $ from Lemma \ref{lem:scalar-extension}. Here, the formalization worked directly with quotients over a relation. However, the relation given by $ R : (x n, m) \sim (x, f(n) \cdot m) $ for all $ a $, is reflexive and transitive, but not necessarily symmetric. Therefore, first the `equivalence closure' $ \bar R $ of $ R $ is taken: the smallest transitive and symmetric relation that contains $ R $. This is then used for the quotient. To construct functions from this, like, the right action of the monoid $ M^\prime $, we use the universal property of the quotient: a function from $ X \times M^\prime $ that sends `related' elements $ (x_1, m_1) $ and $ (x_2, m_2) $ to the same element. However, the quotient is taken over $ \bar R $ instead of $ R $, so it is no longer possible to assume that $ x_2 = x_1 n $ and $ m_1 = f(n) \cdot m_2 $ for some $ n $. In the case of the right $ M^\prime $-action, it sufficed to add the following lemma to the library
\begin{lemma}
  For a relation $ S $ on a set $ A $ and a relation $ T $ on a set $ B $, if for $ f: A \to B $, we have that $ S(a_1, a_2) $ implies $ T(f(a_1), f(a_2)) $ for all $ a_1, a_2 : A $, then $ \bar S(a_1, a_2) $ implies $ \bar T(f(a_1), f(a_2)) $ for all $ a_1, a_2 $.
\end{lemma}
However, overall, working with these quotients in an elementary way was quite a hassle.

The second occurrence of quotients was in the proof of Corollary \ref{cor:karoubi-presheaf}, or the lemma that it is based on. Initially, the statement of the lemma was only about $ \SET $, stating that $ \iota_C \bullet - : [\bar C, \SET] \to [C, \SET] $ was an adjoint equivalence. The proof used coequalizers of functors, like in Lemma \ref{lem:karoubi-equivalence}. Note that coequalizers of set-valued functions are given pointwise by coequalizers in $ \SET $, which are formalized using quotients. Because of this, when simplifying, coq usually reduced the goal to elementary statements about quotients, which somewhat obfuscated the relatively simple nature of the construction. Luckily, the construction could be made agnostic to the specific implementation of coequalizers, by introducing a variable
\[ \mathtt{H : Coequalizers HSET} \]
and using this instead of our known implementation. Since the proofs did not use any elements of the sets in question, it was then trivial to generalize to any category with coequalizers. It turned out that the construction was a special version of left Kan extension, which uses general colimits, and using the existing definition of Kan extension, the proof was made considerably shorter.

In hindsight, it may have been better to replace the quotient $ Y := X \times M / \sim $ by a coequalizer
\begin{center}
  \begin{tikzcd}[sep = huge]
    N \times X \times M \arrow[r, shift left, "{(n, x, m) \mapsto (x n, m)}"] \arrow[r, shift right, "{(n, x, m) \mapsto (x, f(n) \cdot m)}"'] & X \times M \arrow[r, dashed] & Y
  \end{tikzcd},
\end{center}
which is closer to the way this relation is defined. Under the hood, this still takes a quotient over the equivalence closure, but now it is less visible. Here, the universal property of the coequalizer can be used to define functions out of $ Y $, which may be easier to work with than the universal property of the quotient.

The generalization from $ \SET $ to an arbitrary category with coequalizers, which we saw in the case of Karoubi envelopes, can also be done here, but it would take considerably more effort. The problem is that the definitions of monoids and sets with a monoid action really do use the elements of their underlying sets. For example, a monoid $ M $ has a unit $ u $ and multiplication $ \cdot $, such that $ u \cdot m = m = m \cdot u $ and $ m \cdot (n \cdot l) = (m \cdot n) \cdot l $ for all $ m, n, l : M $. The generalization for an object $ M $ in some category $ C $, turns the unit into a function $ u : I \to M $ from the terminal object and the multiplication into a function $ \mu : M \times M $ from the binary product
\footnote{Note that one does not need the full power of binary products or terminal objects to define internal monoids. A category with the product-like structure $ \otimes $ and terminal-like object $ I $ that one needs to define internal monoids, is called a \index{category!monoidal}\textit{monoidal category}.},
such that the following diagrams commute:
\begin{center}
  \begin{tikzcd}
    M \times (M \times M) \arrow[rr, "{\sim}"] \arrow[d, "{\id M \times \mu}"'] & & (M \times M) \times M \arrow[d, "{\mu \times \id M}"]\\
    M \times M \arrow[rd, "{\mu}"'] & & M \times M \arrow[ld, "{\mu}"]\\
    & M
  \end{tikzcd}
  \begin{tikzcd}
    I \times M \arrow[r, "u \times \id M"] \arrow[rd, "\sim"'] & M \times M \arrow[d, "\mu"] & M \times I \arrow[l, "\id M \times u"'] \arrow[ld, "\sim"]\\
    & M
  \end{tikzcd}
\end{center}
Such a generalization of a monoid to a general category is called an \index{monoid!internal}\textit{internal monoid}. In a similar way, sets with a monoid action can be generalized. However, this all would be a lot of work, and elementary reasoning about internal monoids of $ \SET $ is still much more cumbersome than for the ordinary set-based monoids. Also, for the fundamental theorem we only need the ordinary set-based monoids. Therefore, this generalization was not pursued any further.

\section{The formalization of the \texorpdfstring{$ \lambda $}{lambda}-calculus} \label{sec:axiomatic-lambda-calculus}
One of the important classes of objects in this thesis is the class of $ \Lambda $-algebras: algebras for the initial $ \lambda $-theory $ \Lambda $, corresponding to the `pure' $ \lambda $-calculus. Therefore, much of this project required the pure $ \lambda $-calculus to be formalized. Now, in this thesis, it is defined as a quotient of an inductive type. However, in the UniMath library, the use of inductive types (other than a handful of basic ones like \texttt{empty}, \texttt{bool}, \texttt{nat} and \texttt{coprod}) is prohibited
\footnote{This is because allowing inductive types requires a much larger trusted codebase of the proof assistant, and because most of mathematics can be formalized with $ \sum $-types or using initial algebras of functors, instead of inductive and record types.}. Also, as we saw in section \ref{sec:quotients}, working with quotients can be somewhat complicated.

The inductive type approach has a variation, which instead of a quotient uses a higher inductive type to force the required elements to be equal. Unfortunately, even if the use of inductive types was allowed, coq does not support higher inductive types.

Instead, a different, more axiomatic approach was taken. A class in group theory often starts with laying out a couple of axioms, like: ``We have a set $ G $, with a binary operation $ b $, a unary operation $ u $ and an element $ e $. $ b $ is associative, $ u $ is a left and right inverse for $ b $ and $ e $ is a unit for $ b $.'' Afterwards, this structure is usually bundled into the declaration ``Let $ G $ be a group.'' In the same spirit, every section in the formalization that needs the pure $ \lambda $-calculus, starts with ``Let $ L $ be the pure $ \lambda $-calculus with $ \beta $-equality.'' To this end, there is a definition \texttt{lambda\_calculus}, consisting of
\begin{itemize}
  \item A sequence of sets $ (L_n)_n $;
  \item Variables, app, abs, subst:
    \[ \mathtt{var}_{n, i} : L_n, \quad \mathtt{app}_n: L_n \times L_n \to L_n, \quad \mathtt{abs}_n : L_{S n} \to L_n \quad \text{and} \quad \mathtt{subst}_{m, n}: L_m \times L_n^m \to L_n; \]
  \item A couple of identities about the interactions between the constructors. In particular, $ \beta $-equality:
    \[
      \mathtt{subst}(\mathtt{var}_i, t) = t_i, \quad
      \dots, \quad
      \mathtt{app}(\mathtt{abs}(s), t) = \mathtt{subst}(s, (\mathtt{var}_i)_i + (t));
    \]
  \item The induction principle, which coincides with the induction principle that a higher inductive type would have: Given, for all $ t : L_n $, a type $ A_{n, t} $, it is possible to construct, for every $ t : L_n $, an element $ f(t) : A_{n, t} $ by just giving elements and functions, corresponding to the constructors
    \begin{align*}
      f_{\mathtt{var}(n, i)} &:A_{\mathtt{var}_{n, i}}\\
      f_{\mathtt{app}(s, t)} &:A_s \times A_t \to A_{\mathtt{app}(s, t)}\\
      f_{\mathtt{abs}(t)} &:A_t \to A_{\mathtt{abs}(t)}\\
      f_{\mathtt{subst}(s, t)} &:A_s \times A_{t_1} \times \dots \times A_{t_n} \to A_{\mathtt{subst}(s, t)}\\
    \end{align*}
    and by showing that they are compatible with the identities on $ L $. For example, for $ \beta $-equality, this is equality between
    \[ f_{\mathtt{app}}(f_{\mathtt{abs}}(s), t): A_{\mathtt{app}(\mathtt{abs}(s), t)} \quad \text{and} \quad f_{\mathtt{subst}}(s, f_{\mathtt{var}_1}, \dots, f_{\mathtt{var}_n}, t): A_{\mathtt{subst}(s, (\mathtt{var}_i)_i + (t))}. \]
    Note that the left and right hand side live in different types, so the equality can only be stated using a transport over $ \beta $-equality.
  \item A couple of identities about the interaction between induction and the constructors: if $ f $ is defined using the induction principle as above,
  \begin{align*}
    f(\mathtt{var}_i) &= f_{\mathtt{var}_i}\\
    f(\mathtt{app}(s, t)) &= f_{\mathtt{app}(s, t)}(f(s), f(t))\\
    f(\mathtt{abs}(t)) &= f_{\mathtt{abs}(t)}(f(t))\\
    f(\mathtt{subst}(s, t)) &= f_{\mathtt{subst}(s, t)}(f(s), (f(t_i))_i).
  \end{align*}
\end{itemize}

\begin{remark}
  Note that the first three bullets just give $ L $ a $ \lambda $-theory structure. Only by the induction principle, it becomes clear that $ L $ is the (unique) pure $ \lambda $-calculus.
\end{remark}

\begin{remark}
  The induction principle has two common uses: $ A_{n, t} $ can be taken to be a constant type $ A $ (or potentially $ A_n $), in which case induction results in functions $ L_n \to A $. The $ A_{n, t} $ can also be taken to be mere propositions, in which case induction `proves' something about all $ t : L_n $.

  In both cases, the rules about respecting the identities like $ \beta $-equality become simpler: in the first case, the transports disappear, and in the second case, the rules are satisfied automatically.
\end{remark}

\begin{remark}
  Often, the pure $ \lambda $-calculus is defined using just the constructors $ \mathtt{var} $, $ \mathtt{app} $ and $ \mathtt{abs} $. $ \mathtt{subst} $ is usually defined using induction, which gives the identities about the interaction between $ \mathtt{subst} $ and the others for free. However, since this is a definition of the $ \lambda $-calculus with $ \beta $-equality, the induction principle must include a rule about compatibility with the $ \beta $-equality. Since the definition of $ \beta $-equality already uses $ \mathtt{subst} $, it is not possible to define $ \mathtt{subst} $ using induction. This is why it is added as an additional constructor, along with requirements about its interaction with the other constructors. This approach is called \iindex{explicit substitution}.
\end{remark}

\subsection{Propagation of substitution}
Once the pure $ \lambda $-calculus is defined, it is not very hard to give it a $ \lambda $-theory structure, using the induction principle a couple of times to show that some identities are satisfied.

Now, as mentioned in Subsection \ref{subsec:lambda-calculus-operations}, any $ \lambda $-theory allows the operations $ \mathtt{var} $, $ \mathtt{app} $, $ \mathtt{abs} $ and $ \mathtt{subst} $ with the same interaction as for the pure $ \lambda $-calculus. Therefore, given any $ \lambda $-calculus, it is possible to start defining more complicated structures like the $ a \circ b $, $ \langle a, b \rangle $ and $ A \times B $ from Section \ref{sec:retracts-category}, which is indeed done in the original proof of Scott's representation theorem. However the equalities about the interaction between $ \mathtt{subst} $ and the other operations are not definitional, even for the pure $ \lambda $-calculus. Consider the following term (using concatenation for application):
\[ \lambda x_5, x_5 (x_1 x_2 (x_4 x_3)) \bullet (x_1, x_2, x_3, x_1) : L_3. \]
It is not hard to see that this results in $ \lambda x_4, x_4 (x_1 x_2 (x_1 x_3)) $. However, it takes a lot of steps to rewrite this: moving the substitution past the $ \lambda $-abstraction, then into 4 instances of application, and lastly using 5 instances of the interaction between variables and substitution, resulting in a total of 10 rewrites for a seemingly trivial term. It is not unheard of to have 40 of these rewrites consecutively in a proof, and since there is a lot of things to prove, this quickly becomes tedious.

Therefore, a tactic was added to the project. A first version of this tactic was a variation of
\begin{lstlisting}
Ltac reduce_lambda := (
  rewrite subst_var +
  rewrite subst_l_var +
  ...
  rewrite beta_equality
).
\end{lstlisting}
attempting to rewrite (once) with at least one, but possible multiple of the equalities. The statement \texttt{repeat reduce\_lambda} sometimes took a couple of seconds, but this saved a lot of manual work. Still, there was much room for improvement.

Therefore, along with the original proof of Scott's representation theorem, a new version of the tactic, called \texttt{propagate\_subst} has been added. It recursively traverses the $ \lambda $-term in the left-hand side of the goal, checking whether the term matches a form that can be rewritten into something else. It performs the possible rewrites, and also prints these rewrite statements which can replace it. For example, if the goal is
\begin{lstlisting}
('m, (inflate a (inflate b var (stnweq (inr tt)))))  c =
('n, (inflate (a  c) (inflate (b  c) var (stnweq (inr tt)))))
\end{lstlisting}
a call to \texttt{repeat reduce\_lambda} would take about a second, but a call to \texttt{propagate\_subst ()} runs in about $ \frac 1 3 $ of a second and prints
\begin{lstlisting}
refine '(subst_abs _ _ _ @ _).
refine '(_ @ !maponpaths ( x, (abs (app x _))) (inflate_subst _ _ _)).
refine '(_ @ !maponpaths ( x, (abs (app _ (app x _)))) (inflate_subst _ _ _)).
refine '(maponpaths ( x, (abs x)) (subst_app _ _ _ _) @ _).
refine '(maponpaths ( x, (abs (app x _))) (subst_inflate _ _ _) @ _).
refine '(maponpaths ( x, (abs (app _ x))) (subst_app _ _ _ _) @ _).
refine '(maponpaths ( x, (abs (app _ (app x _)))) (subst_inflate _ _ _) @ _).
refine '(maponpaths ( x, (abs (app _ (app _ x)))) (var_subst _ _ _) @ _).
refine '(maponpaths ( x, (abs (app _ (app _ x)))) (extend_tuple_inr _ _ _) @ _).
\end{lstlisting}
Replacing the call to \texttt{propagate\_subst} by these statements, results in the same rewrites, but these only take $ \frac{1}{25} $ of a second.

Now, on top of the speedup, the new tactic is modular and extensible. It is modular, in the sense that some of its parts are also tactics themselves, and can be used for other tactics as well. For example, the \texttt{traverse} tactic, which traverses a $ \lambda $-term in the goal and executes something for every subterm, is also used in a new tactic that is called \texttt{generate\_refine}, which takes a pattern, and for every subterm that matches it, prints a
\begin{lstlisting}
refine '(maponpaths ( x, ... x ...) _ @ _).
\end{lstlisting}
statement, which can be used to quickly generate statements that very precisely rewrite one subterm. The \texttt{propagate\_subst} tactic is also extensible, in the sense that the patterns for both the subterm traversal and the rewrites are kept in a list, which can be extended when new combinators are defined. For example, at the point where the tactics are defined, the traversal only works for the constructors $ \mathtt{var} $, $ \mathtt{app} $, $ \mathtt{abs} $ and $ \mathtt{subst} $, and the rewrites only work for the interactions between these operations. Using this, composition $ a \circ b $ is defined, and so a pattern to branch into $ a $ and $ b $ is added to the traversals, and a rewrite with
\[ (a \circ b) \bullet t = (a \bullet t) \circ (b \bullet t) \]
is added. Progressing through the file, the same is done for combinators like the pair $ (a, b) $, the projection $ \pi_1 $ (including a rewrite $ \pi_1 \langle a, b \rangle = a $), $ \mathtt{curry} $ and $ \mathtt{n\_tuple} $ (consisting of nested pairs).

It would be interesting to see whether parts of these tactics can be generalized. For example, whether it is possible to create an extensible tactic which shows that some type is a homotopy set, or a mere proposition, and prints the coq statements that can replace the tactic call. Or whether it would be possible to generalize \texttt{generate\_refine} to very qukickly generate very fast and precise rewrite statements for large and complicated goals.

\section{The learning curve}
Let me conclude this chapter with a more personal note about formalization. In the past fifteen years, I have worked with a plethora of different programming languages, and I would like to think that in all this time, I have learned a great deal about programming. When I start working with a new language, it takes me a couple of minutes to figure things out, and within an hour, I can probably get a small program up and running, with some help from google. However, when I started working with coq and UniMath, I kind of had to start from scratch again. As it turns out, the naive or direct approach is often not the right one when working with a proof assistant, and of the code that I wrote in the first months, almost nothing remains. Even though every programming language has some sort of learning curve, formalizing in a proof assistant is especially unforgiving: You often have to change an old definition, or start over when you are halfway through a proof, because your current approach gets bogged down further and further with, for example, unintelligible transports. For every line of code that ends up in a pull request, many lines are written and erased again.

For example, if we want to show that two types $ X $ and $ Y $ are equivalent, we need to construct an equivalence, which has the following type:
\[ X \simeq Y := \{ f : X \to Y \mid \forall y, \mathtt{is\_contractible}(\{ x \mid f(x) = y \}) \}. \]
One can view this as having an `isomorphism'
\[ f: X \to Y, \quad g : Y \to X, \quad h_1(x): g(f(x)) = x \quad \text{and} \quad h_2(y) : f(g(y)) = y, \]
but then together with a proof $ h_3 $, showing that applying $ f $ to $ h_1(x) $ is the same as taking $ h_2(f(x)) $. This last part is to ensure that `$ f $ is a weak equivalence' is a mere proposition, even if $ X $ and $ Y $ are not sets: if $ X $ and $ Y $ are not sets, there can be multiple paths $ h_1 $ and $ h_2 $, so $ f $ can be an isomorphism `in multiple ways'. It can be tempting to just close our eyes and start constructing this equivalence part by part. When we then get to defining $ h_3 $, however, it is easy to get stuck, because reasoning about paths is complicated. Luckily, there is a surjection
\[ \mathtt{isweq\_iso} : \mathtt{is\_iso}(f) \twoheadrightarrow \mathtt{isweq}(f), \]
which allows us to construct an isomorphism and then get an equivalence (with $ h_3 $) for free.

Another example: When we want to define a category, usually we start by defining what the objects and morphisms in the category look like, and then constructing a category from that. Since we work in univalent foundations here, we want to show that the category is univalent, so that $ \mathrm{idtoiso}: (a = b) \to (a \cong b) $ is an equivalence. However, if we try to show this directly, the proof quickly accumulates piles of transports, that are very hard to resolve. Instead, there are two common `indirect' approaches:
\begin{itemize}
  \item Either we first construct multiple smaller equivalences, ignoring $ \mathrm{idtoiso} $ for the time being:
    \begin{align*}
      (a = b) &\simeq \{ \text{componentwise equalities between $ a $ and $ b $} \}\\
      &\simeq \{ \text{componentwise isomorphisms between $ a $ and $ b $} \}\\
      &\simeq (a \cong b),
    \end{align*}
    and then use a proof that if we have an equivalence $ f: X \simeq Y $, and we have some $ g : X \to Y $ with $ f(x) = g(x) $ for all $ x : X $, we know that $ g $ is an equivalence as well.
  \item We transform the category into a stack of displayed categories, derive definitions for our objects from this displayed category, and then do the easy proofs that the fibers of every layer are univalent.
\end{itemize}
Both of these approaches are not what we would think of in the first place, but they are the approaches that turn out to work very well. In short, there is a very steep learning curve for working with a proof assistant and it takes a lot of experience to formalize mathematics. Not only to know what kind of tactics are available, but also to get a feeling for the theorems, lemmas and constructions that work well for proving or constructing something.
